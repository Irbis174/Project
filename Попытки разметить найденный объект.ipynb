{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8813f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, Conv2D, MaxPooling2D, BatchNormalization, Dropout\n",
    "import numpy as np\n",
    "import cv2 \n",
    "import os\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0764f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (104,104)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0239faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def сorrecting_data():\n",
    "    x = []\n",
    "    y = []\n",
    "    boxes = []\n",
    "    df = pd.read_csv('_annotations.csv') \n",
    "    df['xmin'] = df['xmin'].apply(lambda x: x // 4)\n",
    "    df['ymin'] = df['ymin'].apply(lambda x: x // 4)\n",
    "    df['xmax'] = df['xmax'].apply(lambda x: x // 4)\n",
    "    df['ymax'] = df['ymax'].apply(lambda x: x // 4)\n",
    "    df['width'] = df['width'].apply(lambda x: x // 4)\n",
    "    df['height'] = df['height'].apply(lambda x: x // 4)\n",
    "    h = df['height']\n",
    "    w = df['width']\n",
    "    for x1,y1,x2,y2 in zip(df['xmin'], list(df['ymin']), \n",
    "                       list(df['xmax']), list(df['ymax'])):\n",
    "        arr = [x1, y1, x2, y2]\n",
    "        boxes.append(arr)\n",
    "    return df, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2a8bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_metrics(df,boxes):\n",
    "    images = []\n",
    "    coordinates = []\n",
    "    \n",
    "    for index, boxes in df.iterrows():\n",
    "        image = cv2.imread(index)\n",
    "        images.append(image)\n",
    "        coordinates.append(boxes)\n",
    "    return images, coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85e3ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Масштабирование и преобразование извлеченных областей\n",
    "def data(images, coordinates, image_size):\n",
    "    scaled_regions = []\n",
    "    labels = []\n",
    "    for image, coords in zip(images, coordinates):\n",
    "        #if x != y != w != h != 0\n",
    "        x, y, w, h = coords\n",
    "        car_region = image[y:y+h, x:x+w]\n",
    "        scaled_region = cv2.resize(car_region, image_size)\n",
    "        scaled_regions.append(scaled_region)\n",
    "        labels.append(1)  # Предполагая, что все извлеченные области - это машины\n",
    "    # Преобразование входных данных в массивы NumPy\n",
    "    scaled_regions = np.array(scaled_regions)\n",
    "    labels = np.array(labels)\n",
    "    return scaled_regions, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19cf79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_trash_preparation(df):\n",
    "    images = []\n",
    "    \n",
    "    for index, boxes in df.iterrows():\n",
    "        image = cv2.imread(index)\n",
    "        images.append(image)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a81c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_trash(images, image_size):\n",
    "    scaled_regions = []\n",
    "    labels = []\n",
    "    for image in images:\n",
    "        scaled_region = cv2.resize(image, image_size)\n",
    "        scaled_regions.append(scaled_region)\n",
    "        labels.append(0)  # Предполагая, что все извлеченные области - это машины\n",
    "    # Преобразование входных данных в массивы NumPy\n",
    "    scaled_regions = np.array(scaled_regions)\n",
    "    labels = np.array(labels)\n",
    "    return scaled_regions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d72e42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trash = pd.DataFrame({'filename': os.listdir('DataSet_cars_train\\\\1_trash')})\n",
    "df_trash = df_trash.set_index('filename')\n",
    "df_trash = df_trash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a62accf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matvey\\Проект\\Project\\DataSet_cars_train\\train_1\n",
      "C:\\Users\\Matvey\\Проект\\Project\n"
     ]
    }
   ],
   "source": [
    "%cd DataSet_cars_train/train_1\n",
    "df_train, boxes_train = сorrecting_data()\n",
    "df_train = df_train.set_index('filename')\n",
    "#df = df_train.astype(str)\n",
    "image_train, image_coords = data_metrics(df_train, boxes_train)\n",
    "%cd ../..\n",
    "# %cd DataSet_cars_valid/valid\n",
    "# df_valid, boxes_valid = сorrecting_data()\n",
    "# df_valid = df_valid.set_index('filename')\n",
    "# # df = df_valid.astype(str)\n",
    "# image_valid, image_coords = data_metrics(df_valid, boxes_valid)\n",
    "# %cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "afc06f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27241"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44476e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14d723a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matvey\\Проект\\Project\\DataSet_cars_train\\1_trash\n",
      "C:\\Users\\Matvey\\Проект\\Project\n"
     ]
    }
   ],
   "source": [
    "%cd DataSet_cars_train/1_trash\n",
    "image_trash = data_trash_preparation(df_trash)\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c68ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "400d84b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_train, car_train = data(image_train,boxes_train, image_size)\n",
    "del image_train\n",
    "del boxes_train\n",
    "region_trash, trash_train = data_trash(image_trash, image_size)\n",
    "del image_trash\n",
    "# region_valid, car_valid = data(image_valid,boxes_valid, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f990d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    Conv2D(128, (3, 3), activation='relu', input_shape=(104, 104, 3)),  # свёрточный слой\n",
    "    #попробуй по 2-3 свёрточных слоя сделать\n",
    "    MaxPooling2D((2, 2), strides=2),  # уменьшение карт признаков\n",
    "    Dropout(0.2),  # отключение 20 процентов нейронов\n",
    "\n",
    "#     Conv2D(128, (3, 3), activation='relu'),  # свёрточный слой\n",
    "#     MaxPooling2D((2, 2), strides=2),  # уменьшение карт признаков\n",
    "#     Dropout(0.2),  # отключение 20 процентов нейронов\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu'),  # свёрточный слой\n",
    "    MaxPooling2D((2, 2), strides=2),  # уменьшение карт признаков\n",
    "    Dropout(0.2),  # отключение 20 процентов нейронов\n",
    "\n",
    "#     Conv2D(256, (3, 3), activation='relu'),  # свёрточный слой\n",
    "#     MaxPooling2D((2, 2), strides=2),  # уменьшеи карт признаков\n",
    "#     Dropout(0.2),  # отключение 20 процентов нейронов\n",
    "\n",
    "    Conv2D(512, (3, 3), activation='relu'),  # свёрточный слой\n",
    "    MaxPooling2D((2, 2), strides=2),  # уменьшение карт признаков\n",
    "    Dropout(0.2),  # отключение 20 процентов нейронов\n",
    "\n",
    "#     Conv2D(512, (3, 3), activation='relu'),  # свёрточный слой\n",
    "#     MaxPooling2D((2, 2), strides=2),  # уменьшение карт признаков\n",
    "#     Dropout(0.2),  # отключение 20 процентов нейронов\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),  # обычные нейроны\n",
    "    Dropout(0.2),  # отключение 20 процентов нейронов\n",
    "    Dense(256, activation='relu'),  # обычные нейроны\n",
    "    Dropout(0.2),  # отключение 20 процентов нейронов\n",
    "    Dense(128, activation='relu'),  # обычные нейроны\n",
    "    Dropout(0.2),  # отключение 20 процентов нейронов\n",
    "    Dense(1, activation='sigmoid'),  # выходной слой\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "822ebcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "937eef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    region_train, car_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "905b0dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, ..., 0, 0, 0])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "715921cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_trash, val_data_trash, train_labels_trash, val_labels_trash = train_test_split(\n",
    "    region_trash, trash_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a01181b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 21792, 10135\n  y sizes: 21792\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_trash\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfod\\lib\\site-packages\\keras\\engine\\training.py:1134\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1128\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_coordinator \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mcoordinator\u001b[38;5;241m.\u001b[39mClusterCoordinator(\n\u001b[0;32m   1129\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), \\\n\u001b[0;32m   1132\u001b[0m      training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1133\u001b[0m   \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1134\u001b[0m   data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m      \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m      \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m      \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m      \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1150\u001b[0m   \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfod\\lib\\site-packages\\keras\\engine\\data_adapter.py:1383\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1382\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfod\\lib\\site-packages\\keras\\engine\\data_adapter.py:1138\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1135\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_value \u001b[38;5;241m=\u001b[39m steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m   1137\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1152\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfod\\lib\\site-packages\\keras\\engine\\data_adapter.py:241\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m inputs \u001b[38;5;241m=\u001b[39m pack_x_y_sample_weight(x, y, sample_weights)\n\u001b[0;32m    240\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mint\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(inputs))\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m--> 241\u001b[0m \u001b[43m_check_data_cardinality\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# If batch_size is not passed but steps is, calculate from the input data.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# Default to 32 for backwards compat.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch_size:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfod\\lib\\site-packages\\keras\\engine\\data_adapter.py:1649\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1646\u001b[0m   msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1647\u001b[0m       label, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)))\n\u001b[0;32m   1648\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1649\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 21792, 10135\n  y sizes: 21792\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "history = model.fit((train_data, train_data_trash), train_labels ,epochs = 10, batch_size = batch_size, validation_data = (val_data, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2db5fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('rofl.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "434c51b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del region_valid\n",
    "del region_train\n",
    "del car_valid\n",
    "del car_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfod",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
